Model has following parameters:

convs.0.bias torch.Size([20])
convs.0.weights torch.Size([20, 1, 5, 5])
convs.0.z_phi torch.Size([20])
convs.3.bias torch.Size([50])
convs.3.weights torch.Size([50, 20, 5, 5])
convs.3.z_phi torch.Size([50])
fcs.0.weights torch.Size([800, 500])
fcs.0.z_phi torch.Size([800])
fcs.0.bias torch.Size([500])
fcs.2.weights torch.Size([500, 10])
fcs.2.z_phi torch.Size([500])
fcs.2.bias torch.Size([10])


train --model=ARMLeNet5 --dataset=mnist --lambas="[.1,.1,.1,.1]" --optimizer=adam --lr=0.001
python main.py test --model=ARMLeNet5 --dataset=mnist --lambas="[.1,.1,.1,.1]" --load_file="checkpoints/ARMLeNet5_2019-06-19 14:27:03/0.model"
test --model=ARMLeNet5 --dataset=mnist --lambas="[.1,.1,.1,.1]" --load_file="checkpoints/ARMLeNet5_2020-03-25 15:50:07/0.model" --use_gpu=False
train --model=ARMLeNet5 --dataset=mnist --lambas="[.1,.1,.1,.1]" --optimizer=adam --lr=0.001 --ar True
test --model=ARMLeNet5 --dataset=mnist --lambas="[.1,.1,.1,.1]" --load_file="checkpoints/ARMLeNet5_2019-06-19 14:27:03/0.model" --use_t_in_testing=False --k=70009

train --model=ARMMLP_toy --dataset=toy_dataset --lambas="[.1,.1,.1,.1]" --optimizer=adam --lr=0.001 --ar True
//3 layer
test --model=ARMMLP_toy --dataset=toy_dataset --lambas="[.1,.1,.1,.1]" --load_file=shap-l0-arm-master/checkpoints/ARMMLP_toy_2020-04-12 22:32:56 --use_t_in_testing=False --k=70009
//2 layer
test --model=ARMMLP_toy --dataset=toy_dataset --lambas="[.1,.1,.1,.1]" --load_file=shap-l0-arm-master/checkpoints/ARMMLP_toy_2020-04-14 23:25:15/10.model --use_t_in_testing=False --k=70009



python main.py train --model=ARMWideResNet --dataset=cifar10 --lambas=.001 --optimizer=momentum --lr=0.1 --schedule_milestone="[60, 120]"
python main.py help




ARM_Conv

we first draw
self.u = torch.Tensor(self.dim_z).uniform_(0, 1)

self.z_phi = Parameter(torch.Tensor(out_channels))
z_phi in  ARM_Conv.py has the length of the number of filters and it seems it is similar to the importance switches


update_phi_gradient - i guess very important function where we actually compute the gradient and we use the formula from ARM
compute the gradient and update:  self.z_phi.grad = e


count_expected_flops_and_l0 function in ARM_Conv does a strange thing of computing expected flops reduction where it somehow sums the values ion z_phi and then myltiplies it by the number of weights in a filter

Then in sample_z we went through a sigmoid with that sample_z
if self.forward_mode (both training and testing), we make z go through a thresholf 0.5 (opt.t) and the ones below are zeroed.


Lenet

Compute loss:

1 Part

in the forward motion, we compute two components of the loss, f1 and f2
f1
We set the forward mode to True
1. we first compute the "score" which is the output of the network for each training image
score = self.score(x) #output of the lenet network (value for each vategory)
2. we compute the crossentropy between this output and the true labels y. this is the first part of the loss, f1
f2
it is the same as f1 only in the forward_mode set as False

For some reason the loss takes into account those two values in the forward mode and not in the forward mode

Then we feed it to
self.update_phi_gradient(f1, f2)


2 Part

regularition

main

both losses are combined in the criterion function in the main

-----

f2 is passed to update_gradient in armconv from Lenet (one level up, from its forward function)
in Lenet update_gradient is called for every layer of Lenet model when running a forward function,
that forward function runs forward and produces a score f1, f2, score is just a "mini-forward" function that just runs through the model
score on the other hand calls the forward function in the armconv, which calls the sample_z,
sample_z produces the vector with zeros and ones, like this [0 0 0 1 1 0..]

Parameters:

opt.t is the value of the threshold where we set all the values less than that value to 0. The values higher than opt.t will stay the same (we don't set them to 1)

e.g. if all the phis are below 0, then all the sigmoid values will be less than 0.5 and then by taking opt.t=0.5, all the values will be 0.

Some results:
[array(10), array(20), array(114), array(73)]
epoch:77,lr:0.001,loss:0.05,val_acc:99.15,prune_rate:95.18
epoch:177,lr:0.001,loss:0.04,val_acc:99.18,prune_rate:96.39
epoch:196,lr:0.001,loss:0.04,val_acc:99.15,prune_rate:96.68