for vgg
-main2vgg_script_vggswitch.py -  a script to run a file multiple times for different arguments
-main2vgg_switch.py - runs switches but for vgg
-main2vgg_switch_v1.py - runs switches but for vgg but for the older version with one less fc and two more conv
-mnist.3L.conv.gpu_switch - run the program for switches and saves the swicthes values to the pt file
-plot_switches - makes a plot out of the pt file saved in the mnist_3L.conv.gpu_switch

for lenet
-results_switch/mnist.3L.conv.gpu_switch_working_really_workingversion.py


Steps:

1. Need to pretrain a model without switches

2. Load the pretrain model and run the training for a few epochs

The S (switch) vector gives you the probability distribution of neuron importances.

Example:

Use pretrained models provided on github:
"models/mnist_conv:10_conv:20_fc:100_fc:25_rel_bn_drop_trainval_modelopt1.0_epo:540_acc:99.27"
"models/fashionmnist_conv:10_conv:20_fc:100_fc:25_rel_bn_drop_trainval_modelopt1.0_epo:62_acc:90.04"


Notes:

to custom architecture add the switch function like that

output=self.c5(output)
if layer=='f5':
    output, Sprime = self.switch_func(output)
output=self.f6(output)

The function:

   def switch_func(self, output):
        phi = f.softplus(self.parameter)
        S = phi / torch.sum(phi)
        Sprime = S
        for i in range(len(Sprime)):
            output[:, i] *= Sprime[i].expand_as(output[:, i])
        return output, Sprime


We learn self.parameter, which are the parameters of Dirichlet distribution, alphas. Or in ECCV paper, Dkl[q(s_l|φ_l)||p(s_l|α_0)], they are actually φ.
And then we either take the mean (which is normalized alpha) (pointtest) or we sample from Dirichlete distribution (say 100 samples) (integral)

integral:

we need to compute: \sum q(s_l) \log p(\Dat|s_l)  //also can be integral but we have samples

q(s_l) is a Dirichlet distribution. We need to sample it n times and sum over those samples.

one could adopt the representation of a $k$-dimensional Dirichlet random variable as a weighted sum of Gamma random variables:


we want to learn 10 switches, we choose 50 samples (say) and create [10,50] tensor, concentration_param
and similarly get beta of the same dimenions.
We feed it to Gamma, we have one Gamma per switch value:

Gamma_obj = Gamma(concentration_param, beta_param)

Then we sample from it and normalize each switch value by the sum of switch vector.
These are our S - switches.



